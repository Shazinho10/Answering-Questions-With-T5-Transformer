# -*- coding: utf-8 -*-
"""Inference.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10wz853Pn52swiHfVId1qzLgt1EjfYLcF
"""

import torch

def answer_generator(tokenizer, model, questions, context):
  input_ids = tokenizer(questions, context, return_tensors='pt').input_ids
  output = model.generate(input_ids)
  print('Answer: ', tokenizer.decode(output[0], skip_special_tokens=True))



